{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAlbfpGF5aUh"
      },
      "source": [
        "# Hands-on with Spark (Structured Streaming)\n",
        "\n",
        "![spark](https://cdn-images-1.medium.com/max/300/1*c8CtvqKJDVUnMoPGujF5fA.png)\n",
        "\n",
        "In the previous lesson, we learnt about Spark SQL, Dataframes and Pandas API. In this lesson, we will continue with the Structured Streaming.\n",
        "\n",
        "Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive.\n",
        "\n",
        "You can use the Dataset/DataFrame API to express streaming aggregations, event-time windows, stream-to-batch joins, etc. The computation is executed on the same optimized Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs. In short, Structured Streaming provides **fast, scalable, fault-tolerant, end-to-end exactly-once** stream processing without the user having to reason about streaming.\n",
        "\n",
        "Internally, by default, Structured Streaming queries are processed using a micro-batch processing engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0M79JSa6Oqv"
      },
      "source": [
        "## Installing and Initializing Spark\n",
        "\n",
        "First, like previously, we'll need to install Spark and its dependencies:\n",
        "\n",
        "1.   Java 8\n",
        "2.   Apache Spark with Hadoop\n",
        "3.   Findspark (used to locate the Spark in the system)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KE17krsq6Rd6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.1 pyspark-shell'\n",
        "\n",
        "# set the options to connect to our Kafka cluster\n",
        "options = {\n",
        "    # \"kafka.sasl.jaas.config\": 'org.apache.kafka.common.security.scram.ScramLoginModule required username=\"YnJhdmUtZmlzaC0xMTQ2MyQSvwXBuLOQsV1W7YffuC8cDaZcA3fKQwakMhnQGgg\" password=\"MDUxNjc4YzEtYzYxNy00NTE1LWEwNWYtMDBhODRlZmE0OGJm\";',\n",
        "    # \"kafka.sasl.mechanism\": \"SCRAM-SHA-256\",\n",
        "    # \"kafka.security.protocol\" : \"SASL_SSL\",\n",
        "    \"kafka.bootstrap.servers\": 'localhost:9092',\n",
        "    \"subscribe\": 'pizza-orders',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ra8Ncwms6UsD"
      },
      "outputs": [],
      "source": [
        "# # findspark is only required if you are using a standalone Spark installation (downloaded tar.gz)\n",
        "# import findspark\n",
        "# findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The below cell may take a few minutes to run. It is slow because:\n",
        "- Package downloading: Your PYSPARK_SUBMIT_ARGS includes a Kafka package that needs to be downloaded from Maven repositories on first run\n",
        "- JVM cold start: Initial JVM startup is always slow\n",
        "- Jupyter overhead: Running in a notebook adds some initialization overhead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lhi1xuPB6VfD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Using incubator modules: jdk.incubator.vector\n",
            ":: loading settings :: url = jar:file:/Users/dsai/miniconda3/envs/kafka/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /Users/dsai/.ivy2.5.2/cache\n",
            "The jars for the packages stored in: /Users/dsai/.ivy2.5.2/jars\n",
            "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fe52d6ce-a9c8-4193-a460-926fa782e773;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.1 in central\n",
            "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.1 in central\n",
            "\tfound org.apache.kafka#kafka-clients;3.9.1 in central\n",
            "\tfound org.lz4#lz4-java;1.8.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.10.7 in central\n",
            "\tfound org.slf4j#slf4j-api;2.0.16 in central\n",
            "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.1 in central\n",
            "\tfound org.apache.hadoop#hadoop-client-api;3.4.1 in central\n",
            "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
            "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 in central\n",
            "\tfound org.apache.commons#commons-pool2;2.12.0 in central\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.13/4.0.1/spark-sql-kafka-0-10_2.13-4.0.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.1!spark-sql-kafka-0-10_2.13.jar (102ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.13/4.0.1/spark-token-provider-kafka-0-10_2.13-4.0.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.1!spark-token-provider-kafka-0-10_2.13.jar (73ms)\n",
            "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parallel-collections_2.13/1.2.0/scala-parallel-collections_2.13-1.2.0.jar ...\n",
            "\t[SUCCESSFUL ] org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0!scala-parallel-collections_2.13.jar (125ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.9.1/kafka-clients-3.9.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.9.1!kafka-clients.jar (424ms)\n",
            "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
            "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (88ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.12.0/commons-pool2-2.12.0.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.12.0!commons-pool2.jar (78ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.4.1/hadoop-client-runtime-3.4.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.4.1!hadoop-client-runtime.jar (4594ms)\n",
            "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
            "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (314ms)\n",
            "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.7/snappy-java-1.1.10.7.jar ...\n",
            "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.7!snappy-java.jar(bundle) (305ms)\n",
            "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.16/slf4j-api-2.0.16.jar ...\n",
            "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.16!slf4j-api.jar (72ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.4.1/hadoop-client-api-3.4.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.4.1!hadoop-client-api.jar (1014ms)\n",
            ":: resolution report :: resolve 10942ms :: artifacts dl 7215ms\n",
            "\t:: modules in use:\n",
            "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
            "\torg.apache.commons#commons-pool2;2.12.0 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-client-runtime;3.4.1 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;3.9.1 from central in [default]\n",
            "\torg.apache.spark#spark-sql-kafka-0-10_2.13;4.0.1 from central in [default]\n",
            "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.1 from central in [default]\n",
            "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 from central in [default]\n",
            "\torg.slf4j#slf4j-api;2.0.16 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.10.7 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   11  |   11  |   11  |   0   ||   11  |   11  |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-fe52d6ce-a9c8-4193-a460-926fa782e773\n",
            "\tconfs: [default]\n",
            "\t11 artifacts copied, 0 already retrieved (62683kB/65ms)\n",
            "25/12/09 14:30:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        }
      ],
      "source": [
        "# We will only use 2 cores below to speed up creation of the spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"LearnSparkStreaming\").master(\"local[2]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "VYTXdMSv6cpC",
        "outputId": "e0817f69-3087-4e2c-e643-5ba3ed477a11"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://10.203.33.55:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v4.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[2]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>LearnSparkStreaming</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x109bebd10>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36cRSgfTNXXA"
      },
      "source": [
        "## Read and Analyze Kafka stream in \"Batch\" Mode\n",
        "\n",
        "Let's start with reading and analyzing our `pizza-orders` kafka topic in the usual \"batch\" mode of Spark SQL and Dataframes. This is akin to the batch queries we did in the previous lesson. In this case we are taking the messages with the earliest to latest offsets of the topic as a single \"batch\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "6fZDMWC26dJ0"
      },
      "outputs": [],
      "source": [
        "pizza_df = spark.read.format('kafka')\\\n",
        "    .options(**options)\\\n",
        "    .load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJLqrFeXOXpG",
        "outputId": "56b04137-b3cc-4231-e23a-ea7f16977956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- key: binary (nullable = true)\n",
            " |-- value: binary (nullable = true)\n",
            " |-- topic: string (nullable = true)\n",
            " |-- partition: integer (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- timestampType: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pizza_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+------------+---------+------+--------------------+-------------+\n",
            "|                 key|               value|       topic|partition|offset|           timestamp|timestampType|\n",
            "+--------------------+--------------------+------------+---------+------+--------------------+-------------+\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     0|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     1|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     2|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     3|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     4|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     5|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     6|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     7|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     8|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     9|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|    10|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|    11|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|    12|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|    13|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|    14|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|    15|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|    16|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|    17|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|    18|2025-12-09 14:10:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|    19|2025-12-09 14:10:...|            0|\n",
            "+--------------------+--------------------+------------+---------+------+--------------------+-------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "pizza_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5--uyDySFSEB",
        "outputId": "30d90a0a-8d05-4ae7-8ad6-dcdba0677464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|                 key|               value|\n",
            "+--------------------+--------------------+\n",
            "|{\"shop\": \"Circula...|{\"id\": 0, \"shop\":...|\n",
            "|{\"shop\": \"Ill Mak...|{\"id\": 1, \"shop\":...|\n",
            "|{\"shop\": \"Luigis ...|{\"id\": 2, \"shop\":...|\n",
            "|{\"shop\": \"Luigis ...|{\"id\": 3, \"shop\":...|\n",
            "|{\"shop\": \"Luigis ...|{\"id\": 4, \"shop\":...|\n",
            "|{\"shop\": \"Mammami...|{\"id\": 0, \"shop\":...|\n",
            "|{\"shop\": \"Luigis ...|{\"id\": 1, \"shop\":...|\n",
            "|{\"shop\": \"Its-a m...|{\"id\": 2, \"shop\":...|\n",
            "|{\"shop\": \"Marios ...|{\"id\": 3, \"shop\":...|\n",
            "|{\"shop\": \"Mammami...|{\"id\": 4, \"shop\":...|\n",
            "|{\"shop\": \"Circula...|{\"id\": 5, \"shop\":...|\n",
            "|{\"shop\": \"Marios ...|{\"id\": 6, \"shop\":...|\n",
            "|{\"shop\": \"Marios ...|{\"id\": 7, \"shop\":...|\n",
            "|{\"shop\": \"Circula...|{\"id\": 8, \"shop\":...|\n",
            "|{\"shop\": \"Marios ...|{\"id\": 9, \"shop\":...|\n",
            "|{\"shop\": \"Marios ...|{\"id\": 10, \"shop\"...|\n",
            "|{\"shop\": \"Mammami...|{\"id\": 11, \"shop\"...|\n",
            "|{\"shop\": \"Marios ...|{\"id\": 12, \"shop\"...|\n",
            "|{\"shop\": \"Mammami...|{\"id\": 13, \"shop\"...|\n",
            "|{\"shop\": \"Circula...|{\"id\": 14, \"shop\"...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "pizza_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "m_zjPQ6_FUVE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import from_json, col\n",
        "from pyspark.sql.types import StringType, IntegerType, LongType, DoubleType, StructType, ArrayType, StructField"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "owyJGHliQdBG"
      },
      "outputs": [],
      "source": [
        "pizza_schema = StructType([\n",
        "  StructField(\"pizzaName\", StringType()),\n",
        "  StructField(\"additionalToppings\", ArrayType(StringType())),\n",
        "])\n",
        "\n",
        "order_schema = StructType([\n",
        "  StructField(\"address\", StringType()),\n",
        "  StructField(\"id\", IntegerType()),\n",
        "  StructField(\"name\", StringType()),\n",
        "  StructField(\"phoneNumber\", StringType()),\n",
        "  StructField(\"shop\", StringType()),\n",
        "  StructField(\"cost\", DoubleType()),\n",
        "  StructField(\"pizzas\", ArrayType(pizza_schema)),\n",
        "  StructField(\"timestamp\", LongType()),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "8_KAqsS7Q4_5"
      },
      "outputs": [],
      "source": [
        "parsed_df = pizza_df.select(\"timestamp\", from_json(col(\"value\").cast(\"string\"), order_schema).alias(\"value\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7MWOtWPRM-0",
        "outputId": "af678ae7-e0be-4117-88b9-27cdd9fd8e3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- value: struct (nullable = true)\n",
            " |    |-- address: string (nullable = true)\n",
            " |    |-- id: integer (nullable = true)\n",
            " |    |-- name: string (nullable = true)\n",
            " |    |-- phoneNumber: string (nullable = true)\n",
            " |    |-- shop: string (nullable = true)\n",
            " |    |-- cost: double (nullable = true)\n",
            " |    |-- pizzas: array (nullable = true)\n",
            " |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |-- pizzaName: string (nullable = true)\n",
            " |    |    |    |-- additionalToppings: array (nullable = true)\n",
            " |    |    |    |    |-- element: string (containsNull = true)\n",
            " |    |-- timestamp: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "parsed_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mio4VUyERh1H",
        "outputId": "36c7f7d8-3f03-41dd-9425-5859d45bd940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|timestamp              |value                                                                                                                                                                                                                                                                                                                                          |\n",
            "+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|2025-12-09 14:10:17.277|{21087 Calvin Plains\\nJonesland, NY 76392, 0, Jessica Smith, 001-701-915-3000, Circular Pi Pizzeria, 9.98, [{Margherita, [ü´í olives, üßÖ onion]}], 1765260617264}                                                                                                                                                                               |\n",
            "|2025-12-09 14:10:18.281|{19721 Drew Key\\nNew Donaldport, NH 05690, 1, Roger Brown, 475-943-3780x8105, Ill Make You a Pizza You Cant Refuse, 2.61, [{Marinara, [üçç pineapple, üçå banana, üå∂Ô∏è hot pepper]}, {Marinara, [ü´í olives, üßÄ blue cheese, ü´í olives]}, {Margherita, [ü•ì bacon]}, {Peperoni, [üçå banana, üçç pineapple]}, {Peperoni, [üçÖ tomato]}], 1765260618279}|\n",
            "|2025-12-09 14:10:20.283|{278 Phillips Crossing Apt. 661\\nPort Maryfurt, GU 86535, 2, Matthew Williams, 924-927-0965x759, Luigis Pizza, 4.61, [{Salami, [ü´ë green peppers]}, {Salami, [üßÑ garlic, üå∂Ô∏è hot pepper, üßÖ onion]}, {Mari & Monti, []}], 1765260620283}                                                                                                       |\n",
            "|2025-12-09 14:10:21.287|{547 Brenda Walks\\nLake Ronaldborough, CT 52490, 3, Corey Rodriguez, 001-629-242-5711x630, Luigis Pizza, 43.33, [{Margherita, [ü´ë green peppers]}, {Mari & Monti, [ü´ë green peppers, üßÄ blue cheese, üßÄ blue cheese]}], 1765260621286}                                                                                                         |\n",
            "|2025-12-09 14:10:22.288|{752 Brandon Expressway Apt. 815\\nNorth Elizabeth, AR 84161, 4, Stephen Stevens, 253.786.2934, Luigis Pizza, 42.51, [{Diavola, [ü´ë green peppers]}], 1765260622288}                                                                                                                                                                            |\n",
            "|2025-12-09 14:10:34.592|{144 Ross Port\\nAliciafurt, KY 28535, 0, Ricky Sexton, 8667536484, Mammamia Pizza, 13.93, [{Peperoni, [ü•ì bacon]}, {Marinara, []}, {Salami, [üßÄ blue cheese, ü•ì bacon, üßÄ blue cheese]}], 1765260634592}                                                                                                                                       |\n",
            "|2025-12-09 14:10:35.598|{49595 Gary Port Suite 707\\nNorth Michaelfort, CA 18853, 1, Russell Sanders, (873)972-6401x764, Luigis Pizza, 11.18, [{Salami, [üå∂Ô∏è hot pepper, üßÑ garlic]}, {Marinara, [üçÖ tomato, üßÖ onion, üçì strawberry]}, {Marinara, [üçç pineapple, üçì strawberry, üßÖ onion]}], 1765260635597}                                                            |\n",
            "|2025-12-09 14:10:37.603|{4189 Madison Glens Suite 206\\nLake Vickiborough, TN 20792, 2, Stephanie Rivera, (544)795-9493x841, Its-a me! Mario Pizza!, 4.33, [{Marinara, []}], 1765260637603}                                                                                                                                                                             |\n",
            "|2025-12-09 14:10:38.607|{77289 Dustin Villages\\nLake Frank, NY 99386, 3, Kelli Hamilton DDS, 244-779-3581, Marios Pizza, 40.22, [{Peperoni, [üçç pineapple, üçç pineapple, üßÑ garlic]}, {Margherita, [ü´ë green peppers, ü´ë green peppers, ü´ë green peppers]}, {Salami, []}, {Mari & Monti, []}, {Peperoni, []}], 1765260638607}                                          |\n",
            "|2025-12-09 14:10:40.612|{8697 Anthony Valley\\nPort Kellymouth, FL 64221, 4, Patricia Castaneda, 328-798-9970x51560, Mammamia Pizza, 37.91, [{Margherita, [üçÖ tomato, üßÑ garlic, üßÑ garlic]}], 1765260640612}                                                                                                                                                           |\n",
            "|2025-12-09 14:10:41.618|{409 Ward Stream\\nKeithchester, WI 48718, 5, Marilyn Steele, 5594562103, Circular Pi Pizzeria, 25.08, [{Peperoni, [ü´ë green peppers]}], 1765260641618}                                                                                                                                                                                         |\n",
            "|2025-12-09 14:10:42.623|{272 Tammy Plains Suite 337\\nMaryburgh, MI 97726, 6, Adam Edwards, 718-690-3899x582, Marios Pizza, 34.19, [{Margherita, [üçÖ tomato, üßÖ onion]}, {Diavola, []}, {Mari & Monti, [üêü tuna]}, {Margherita, [ü´í olives, üå∂Ô∏è hot pepper, ü•ö egg]}, {Salami, [üçì strawberry]}], 1765260642623}                                                        |\n",
            "|2025-12-09 14:10:44.629|{3156 Dawson Knoll\\nKelleyborough, PA 62488, 7, Sarah Moreno, 001-399-977-8759x18692, Marios Pizza, 22.22, [{Peperoni, [üêü tuna, üçÖ tomato]}], 1765260644628}                                                                                                                                                                                  |\n",
            "|2025-12-09 14:10:46.635|{0259 Brandon Valley\\nRogermouth, ME 78143, 8, Megan Owens, 881.687.1132x952, Circular Pi Pizzeria, 24.37, [{Mari & Monti, []}], 1765260646635}                                                                                                                                                                                                |\n",
            "|2025-12-09 14:10:47.639|{797 Shaw Junction\\nLisachester, MO 34535, 9, Michelle Orr, (566)404-9945, Marios Pizza, 29.55, [{Margherita, [üêü tuna, ü´ë green peppers]}], 1765260647639}                                                                                                                                                                                    |\n",
            "|2025-12-09 14:10:48.642|{113 Reeves Oval\\nChanbury, UT 40179, 10, Katherine Wright, 922-262-0398x5836, Marios Pizza, 23.69, [{Margherita, [üßÑ garlic, ü•ö egg, ü´ë green peppers]}, {Peperoni, [ü´í olives]}, {Diavola, [üßÑ garlic, ü•ö egg]}, {Margherita, []}], 1765260648642}                                                                                           |\n",
            "|2025-12-09 14:10:49.649|{246 Ramirez Island Apt. 173\\nWrightland, KY 20970, 11, Erin Roth, 671.340.8222x66282, Mammamia Pizza, 23.3, [{Peperoni, [ü•ö egg, üçÖ tomato]}, {Peperoni, [üå∂Ô∏è hot pepper, üêü tuna, üçç pineapple]}], 1765260649648}                                                                                                                            |\n",
            "|2025-12-09 14:10:51.655|{38005 Young Track Apt. 667\\nRiverashire, NC 33178, 12, Sandra Hoffman, (863)238-8235, Marios Pizza, 7.91, [{Margherita, [ü´ë green peppers]}], 1765260651654}                                                                                                                                                                                  |\n",
            "|2025-12-09 14:10:52.656|{6306 Rodriguez Loop\\nPort Grant, CA 94996, 13, Patricia Deleon, 001-374-979-0049, Mammamia Pizza, 0.75, [{Marinara, [üêü tuna, ü´ë green peppers]}, {Diavola, [üçÖ tomato, üçç pineapple]}, {Diavola, []}, {Margherita, []}, {Marinara, [ü´í olives, ü•ì bacon, üçì strawberry]}], 1765260652656}                                                    |\n",
            "|2025-12-09 14:10:53.66 |{67042 Melissa Overpass\\nWest Johnmouth, HI 35018, 14, Christopher Chang, 286-602-4805, Circular Pi Pizzeria, 7.6, [{Diavola, []}, {Salami, [ü´ë green peppers]}, {Peperoni, []}], 1765260653659}                                                                                                                                               |\n",
            "+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "parsed_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdYUKmrygE7_"
      },
      "source": [
        "We can use _dot notation_ to select the field within a `Struct`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt_TUX3_edrK",
        "outputId": "8907d09f-371c-4c04-8912-a91348b40c3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "| cost|\n",
            "+-----+\n",
            "| 9.98|\n",
            "| 2.61|\n",
            "| 4.61|\n",
            "|43.33|\n",
            "|42.51|\n",
            "|13.93|\n",
            "|11.18|\n",
            "| 4.33|\n",
            "|40.22|\n",
            "|37.91|\n",
            "|25.08|\n",
            "|34.19|\n",
            "|22.22|\n",
            "|24.37|\n",
            "|29.55|\n",
            "|23.69|\n",
            "| 23.3|\n",
            "| 7.91|\n",
            "| 0.75|\n",
            "|  7.6|\n",
            "+-----+\n",
            "only showing top 20 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "parsed_df.select(\"value.cost\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgfNZXpVgXz_"
      },
      "source": [
        "Computing the \"total revenue\" per shop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqZHVVFggUYv",
        "outputId": "5c140144-c405-44d5-ff32-e38bdd0cab0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------+-----------------------+\n",
            "|shop                                |sum(value.cost AS cost)|\n",
            "+------------------------------------+-----------------------+\n",
            "|Ill Make You a Pizza You Cant Refuse|326.65                 |\n",
            "|Luigis Pizza                        |369.75                 |\n",
            "|Mammamia Pizza                      |297.33                 |\n",
            "|Its-a me! Mario Pizza!              |321.07000000000005     |\n",
            "|Marios Pizza                        |306.28                 |\n",
            "|Circular Pi Pizzeria                |239.81                 |\n",
            "+------------------------------------+-----------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "parsed_df.groupBy(\"value.shop\").sum(\"value.cost\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GuvYqRChU_v"
      },
      "source": [
        "> 1. Count the no. of orders by shop.\n",
        "> 2. Compute the avg revenue by shop and sort by highest to lowest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "fEuCpW_Pgk1Q"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import min, max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8IhcetFiY24",
        "outputId": "7af16123-be01-4746-e028-2d0f66212964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------+-----------------------+\n",
            "|min(timestamp)         |max(timestamp)         |\n",
            "+-----------------------+-----------------------+\n",
            "|2025-12-09 14:10:17.277|2025-12-09 14:37:26.209|\n",
            "+-----------------------+-----------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "parsed_df.select(min(\"timestamp\"), max(\"timestamp\")).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MDIcbPs74B3"
      },
      "source": [
        "## Read and Analyze Kafka stream in \"Streaming\" Mode\n",
        "\n",
        "The key idea in Structured Streaming is to treat a live data stream as a table that is being continuously appended. This leads to a new stream processing model that is very similar to a batch processing model. You will express your streaming computation as standard batch-like query as on a static table, and Spark runs it as an incremental query on the *unbounded input table*. Let‚Äôs understand this model in more detail.\n",
        "\n",
        "Consider the input data stream as the ‚ÄúInput Table‚Äù. Every data item that is arriving on the stream is like a new row being appended to the Input Table.\n",
        "\n",
        "![concept](https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png)\n",
        "\n",
        "A query on the input will generate the ‚ÄúResult Table‚Äù. Every trigger interval (say, every 1 second), new rows get appended to the Input Table, which eventually updates the Result Table. Whenever the result table gets updated, we would want to write the changed result rows to an external sink.\n",
        "\n",
        "![result table](https://spark.apache.org/docs/latest/img/structured-streaming-model.png)\n",
        "\n",
        "To illustrate the use of this model, let‚Äôs understand the model in context of a word count model. The first lines DataFrame is the input table, and the final wordCounts DataFrame is the result table. Note that the query on streaming lines DataFrame to generate wordCounts is exactly the same as it would be a static DataFrame. However, when this query is started, Spark will continuously check for new data from the socket connection. If there is new data, Spark will run an ‚Äúincremental‚Äù query that combines the previous running counts with the new data to compute updated counts, as shown below.\n",
        "\n",
        "![example](https://spark.apache.org/docs/latest/img/structured-streaming-example-model.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9A-q7BY9F_F"
      },
      "source": [
        "Event-time is the time embedded in the data itself. For many applications, you may want to operate on this event-time.\n",
        "\n",
        "For example, if you want to get the number of events generated by IoT devices every minute, then you probably want to use the time when the data was generated (that is, event-time in the data), rather than the time Spark receives them.\n",
        "\n",
        "This event-time is very naturally expressed in this model ‚Äì each event from the devices is a row in the table, and event-time is a column value in the row. This allows window-based aggregations (e.g. number of events every minute) to be just a special type of grouping and aggregation on the event-time column ‚Äì each time window is a group and each row can belong to multiple windows/groups. Therefore, such event-time-window-based aggregation queries can be defined consistently on both a static dataset (e.g. from collected device events logs) as well as on a data stream, making the life of the user much easier.\n",
        "\n",
        "Furthermore, this model naturally handles data that has arrived later than expected based on its event-time. Since Spark is updating the Result Table, it has full control over updating old aggregates when there is late data, as well as cleaning up old aggregates to limit the size of intermediate state data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "TKfF1Mc9STFX"
      },
      "outputs": [],
      "source": [
        "pizza_df = spark.readStream.format('kafka')\\\n",
        "    .options(**options)\\\n",
        "    .load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss8wgrNDkwuW",
        "outputId": "d0f3d31d-d229-46f7-db9b-9a893ca8f44b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pizza_df.isStreaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grlYCt8Ujg3G",
        "outputId": "3048feba-75ec-40bd-9f1f-6b9bd8788622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- key: binary (nullable = true)\n",
            " |-- value: binary (nullable = true)\n",
            " |-- topic: string (nullable = true)\n",
            " |-- partition: integer (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- timestampType: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pizza_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "G_vL4jkTjl2v"
      },
      "outputs": [],
      "source": [
        "parsed_df = pizza_df.select(\"timestamp\", from_json(col(\"value\").cast(\"string\"), order_schema).alias(\"value\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1ETDCxskFZ9",
        "outputId": "9b7b3a27-13c7-447a-d5cd-2a02d0d55892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- value: struct (nullable = true)\n",
            " |    |-- address: string (nullable = true)\n",
            " |    |-- id: integer (nullable = true)\n",
            " |    |-- name: string (nullable = true)\n",
            " |    |-- phoneNumber: string (nullable = true)\n",
            " |    |-- shop: string (nullable = true)\n",
            " |    |-- cost: double (nullable = true)\n",
            " |    |-- pizzas: array (nullable = true)\n",
            " |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |-- pizzaName: string (nullable = true)\n",
            " |    |    |    |-- additionalToppings: array (nullable = true)\n",
            " |    |    |    |    |-- element: string (containsNull = true)\n",
            " |    |-- timestamp: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "parsed_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "zY8eDSs9kLan",
        "outputId": "c4ae7110-9517-4f0d-bf8a-d767fb4c8c55"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/09 14:40:36 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/76/c58h26g508d9m8jvpnjj09dr0000gr/T/temporary-2cde2791-e9bc-492b-ad5b-4728be32ba50. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
            "25/12/09 14:40:36 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+---------+-----+\n",
            "|timestamp|value|\n",
            "+---------+-----+\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = parsed_df \\\n",
        "    .writeStream \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "# query.awaitTermination() # stops the script from exiting. But this is not required in Jupyter because the kernel stays alive. In real Spark applications, you need awaitTermination() to keep the job running.\n",
        "# query.isActive\n",
        "# query.recentProgress\n",
        "# query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/09 14:41:03 WARN DAGScheduler: Failed to cancel job group c04356b8-6324-4668-bfb2-c803e2c92f58. Cannot find active jobs for it.\n",
            "25/12/09 14:41:03 WARN DAGScheduler: Failed to cancel job group c04356b8-6324-4668-bfb2-c803e2c92f58. Cannot find active jobs for it.\n"
          ]
        }
      ],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{\n",
              "   \"id\" : \"35bb1dfd-1950-4daa-b808-5f3cf448b389\",\n",
              "   \"runId\" : \"c04356b8-6324-4668-bfb2-c803e2c92f58\",\n",
              "   \"name\" : null,\n",
              "   \"timestamp\" : \"2025-12-09T06:40:37.171Z\",\n",
              "   \"batchId\" : 0,\n",
              "   \"batchDuration\" : 754,\n",
              "   \"numInputRows\" : 0,\n",
              "   \"inputRowsPerSecond\" : 0.0,\n",
              "   \"processedRowsPerSecond\" : 0.0,\n",
              "   \"durationMs\" : {\n",
              "     \"addBatch\" : 33,\n",
              "     \"commitOffsets\" : 223,\n",
              "     \"getBatch\" : 1,\n",
              "     \"latestOffset\" : 330,\n",
              "     \"queryPlanning\" : 6,\n",
              "     \"triggerExecution\" : 754,\n",
              "     \"walCommit\" : 161\n",
              "   },\n",
              "   \"stateOperators\" : [ ],\n",
              "   \"sources\" : [ {\n",
              "     \"description\" : \"KafkaV2[Subscribe[pizza-orders]]\",\n",
              "     \"startOffset\" : null,\n",
              "     \"endOffset\" : {\n",
              "       \"pizza-orders\" : {\n",
              "         \"0\" : 80\n",
              "       }\n",
              "     },\n",
              "     \"latestOffset\" : {\n",
              "       \"pizza-orders\" : {\n",
              "         \"0\" : 80\n",
              "       }\n",
              "     },\n",
              "     \"numInputRows\" : 0,\n",
              "     \"inputRowsPerSecond\" : 0.0,\n",
              "     \"processedRowsPerSecond\" : 0.0,\n",
              "     \"metrics\" : {\n",
              "       \"avgOffsetsBehindLatest\" : \"0.0\",\n",
              "       \"maxOffsetsBehindLatest\" : \"0\",\n",
              "       \"minOffsetsBehindLatest\" : \"0\"\n",
              "     }\n",
              "   } ],\n",
              "   \"sink\" : {\n",
              "     \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@5be4ffd6\",\n",
              "     \"numOutputRows\" : 0\n",
              "   }\n",
              " },\n",
              " {\n",
              "   \"id\" : \"35bb1dfd-1950-4daa-b808-5f3cf448b389\",\n",
              "   \"runId\" : \"c04356b8-6324-4668-bfb2-c803e2c92f58\",\n",
              "   \"name\" : null,\n",
              "   \"timestamp\" : \"2025-12-09T06:40:47.934Z\",\n",
              "   \"batchId\" : 1,\n",
              "   \"batchDuration\" : 6,\n",
              "   \"numInputRows\" : 0,\n",
              "   \"inputRowsPerSecond\" : 0.0,\n",
              "   \"processedRowsPerSecond\" : 0.0,\n",
              "   \"durationMs\" : {\n",
              "     \"latestOffset\" : 5,\n",
              "     \"triggerExecution\" : 6\n",
              "   },\n",
              "   \"stateOperators\" : [ ],\n",
              "   \"sources\" : [ {\n",
              "     \"description\" : \"KafkaV2[Subscribe[pizza-orders]]\",\n",
              "     \"startOffset\" : {\n",
              "       \"pizza-orders\" : {\n",
              "         \"0\" : 80\n",
              "       }\n",
              "     },\n",
              "     \"endOffset\" : {\n",
              "       \"pizza-orders\" : {\n",
              "         \"0\" : 80\n",
              "       }\n",
              "     },\n",
              "     \"latestOffset\" : {\n",
              "       \"pizza-orders\" : {\n",
              "         \"0\" : 80\n",
              "       }\n",
              "     },\n",
              "     \"numInputRows\" : 0,\n",
              "     \"inputRowsPerSecond\" : 0.0,\n",
              "     \"processedRowsPerSecond\" : 0.0,\n",
              "     \"metrics\" : {\n",
              "       \"avgOffsetsBehindLatest\" : \"0.0\",\n",
              "       \"maxOffsetsBehindLatest\" : \"0\",\n",
              "       \"minOffsetsBehindLatest\" : \"0\"\n",
              "     }\n",
              "   } ],\n",
              "   \"sink\" : {\n",
              "     \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@5be4ffd6\",\n",
              "     \"numOutputRows\" : 0\n",
              "   }\n",
              " },\n",
              " {\n",
              "   \"id\" : \"35bb1dfd-1950-4daa-b808-5f3cf448b389\",\n",
              "   \"runId\" : \"c04356b8-6324-4668-bfb2-c803e2c92f58\",\n",
              "   \"name\" : null,\n",
              "   \"timestamp\" : \"2025-12-09T06:40:57.941Z\",\n",
              "   \"batchId\" : 1,\n",
              "   \"batchDuration\" : 6,\n",
              "   \"numInputRows\" : 0,\n",
              "   \"inputRowsPerSecond\" : 0.0,\n",
              "   \"processedRowsPerSecond\" : 0.0,\n",
              "   \"durationMs\" : {\n",
              "     \"latestOffset\" : 6,\n",
              "     \"triggerExecution\" : 6\n",
              "   },\n",
              "   \"stateOperators\" : [ ],\n",
              "   \"sources\" : [ {\n",
              "     \"description\" : \"KafkaV2[Subscribe[pizza-orders]]\",\n",
              "     \"startOffset\" : {\n",
              "       \"pizza-orders\" : {\n",
              "         \"0\" : 80\n",
              "       }\n",
              "     },\n",
              "     \"endOffset\" : {\n",
              "       \"pizza-orders\" : {\n",
              "         \"0\" : 80\n",
              "       }\n",
              "     },\n",
              "     \"latestOffset\" : {\n",
              "       \"pizza-orders\" : {\n",
              "         \"0\" : 80\n",
              "       }\n",
              "     },\n",
              "     \"numInputRows\" : 0,\n",
              "     \"inputRowsPerSecond\" : 0.0,\n",
              "     \"processedRowsPerSecond\" : 0.0,\n",
              "     \"metrics\" : {\n",
              "       \"avgOffsetsBehindLatest\" : \"0.0\",\n",
              "       \"maxOffsetsBehindLatest\" : \"0\",\n",
              "       \"minOffsetsBehindLatest\" : \"0\"\n",
              "     }\n",
              "   } ],\n",
              "   \"sink\" : {\n",
              "     \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@5be4ffd6\",\n",
              "     \"numOutputRows\" : 0\n",
              "   }\n",
              " }]"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query.recentProgress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### In the next few cells, we will provide a complete example of counting the number of orders per pizza shop, as the orders stream in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<pyspark.sql.streaming.query.StreamingQuery at 0x1424aab10>]"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# First check if any streams are active\n",
        "spark.streams.active"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stopping query: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/09 14:41:26 WARN DAGScheduler: Failed to cancel job group 3fbcab69-26cf-42f2-9724-1839c75d222f. Cannot find active jobs for it.\n",
            "25/12/09 14:41:26 WARN DAGScheduler: Failed to cancel job group 3fbcab69-26cf-42f2-9724-1839c75d222f. Cannot find active jobs for it.\n"
          ]
        }
      ],
      "source": [
        "# If there are any active streams, stop them\n",
        "for q in spark.streams.active:\n",
        "    print(f\"Stopping query: {q.name}\")\n",
        "    q.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/09 14:41:34 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/76/c58h26g508d9m8jvpnjj09dr0000gr/T/temporary-269faea1-7544-40a3-9997-b335c2173c63. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
            "25/12/09 14:41:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+----+-----+\n",
            "|shop|count|\n",
            "+----+-----+\n",
            "+----+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 1\n",
            "-------------------------------------------\n",
            "+--------------------+-----+\n",
            "|                shop|count|\n",
            "+--------------------+-----+\n",
            "|Circular Pi Pizzeria|    1|\n",
            "+--------------------+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 2\n",
            "-------------------------------------------\n",
            "+--------------------+-----+\n",
            "|                shop|count|\n",
            "+--------------------+-----+\n",
            "|Ill Make You a Pi...|    1|\n",
            "|        Luigis Pizza|    1|\n",
            "|Its-a me! Mario P...|    1|\n",
            "|Circular Pi Pizzeria|    2|\n",
            "+--------------------+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 3\n",
            "-------------------------------------------\n",
            "+--------------------+-----+\n",
            "|                shop|count|\n",
            "+--------------------+-----+\n",
            "|Ill Make You a Pi...|    1|\n",
            "|        Luigis Pizza|    1|\n",
            "|Its-a me! Mario P...|    2|\n",
            "|Circular Pi Pizzeria|    2|\n",
            "+--------------------+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 4\n",
            "-------------------------------------------\n",
            "+--------------------+-----+\n",
            "|                shop|count|\n",
            "+--------------------+-----+\n",
            "|Ill Make You a Pi...|    1|\n",
            "|        Luigis Pizza|    1|\n",
            "|      Mammamia Pizza|    2|\n",
            "|Its-a me! Mario P...|    3|\n",
            "|Circular Pi Pizzeria|    3|\n",
            "+--------------------+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/dsai/miniconda3/envs/kafka/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/dsai/miniconda3/envs/kafka/lib/python3.11/site-packages/py4j/clientserver.py\", line 535, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/dsai/miniconda3/envs/kafka/lib/python3.11/socket.py\", line 718, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# outputMode(\"complete\") rewrites all aggregated results every trigger ‚Äî good for full snapshots.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# outputMode(\"update\") - only updated rows are written\u001b[39;00m\n\u001b[32m     12\u001b[39m query = shop_counts \\\n\u001b[32m     13\u001b[39m     .writeStream \\\n\u001b[32m     14\u001b[39m     .outputMode(\u001b[33m\"\u001b[39m\u001b[33mcomplete\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     15\u001b[39m     .format(\u001b[33m\"\u001b[39m\u001b[33mconsole\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     16\u001b[39m     .start()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kafka/lib/python3.11/site-packages/pyspark/sql/streaming/query.py:225\u001b[39m, in \u001b[36mStreamingQuery.awaitTermination\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jsq.awaitTermination(\u001b[38;5;28mint\u001b[39m(timeout * \u001b[32m1000\u001b[39m))\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kafka/lib/python3.11/site-packages/py4j/java_gateway.py:1361\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1354\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kafka/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kafka/lib/python3.11/site-packages/py4j/clientserver.py:535\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m         answer = smart_decode(\u001b[38;5;28mself\u001b[39m.stream.readline()[:-\u001b[32m1\u001b[39m])\n\u001b[32m    536\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    537\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    538\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kafka/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Complete example with counting\n",
        "pizza_df = spark.readStream.format('kafka')\\\n",
        "    .options(**options)\\\n",
        "    .load()\n",
        "\n",
        "parsed_df = pizza_df.select(\"timestamp\", from_json(col(\"value\").cast(\"string\"), order_schema).alias(\"value\")) #.groupBy(\"value.shop\").count()\n",
        "\n",
        "shop_counts = parsed_df.groupBy(\"value.shop\").count()\n",
        "\n",
        "# outputMode(\"complete\") rewrites all aggregated results every trigger ‚Äî good for full snapshots.\n",
        "# outputMode(\"update\") - only updated rows are written\n",
        "query = shop_counts \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/09 14:46:08 WARN DAGScheduler: Failed to cancel job group 72e608c8-491d-4dd6-bfbb-f7914023da37. Cannot find active jobs for it.\n",
            "25/12/09 14:46:08 WARN DAGScheduler: Failed to cancel job group 72e608c8-491d-4dd6-bfbb-f7914023da37. Cannot find active jobs for it.\n"
          ]
        }
      ],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You have come to the end of this exercise.\n",
        "\n",
        "To delete the Kafka topic `pizza-orders`, use the command below in your terminal:\n",
        "\n",
        "`./kafka-topics.sh --delete --topic pizza-orders --bootstrap-server localhost:9092`\n",
        "\n",
        "To exit Kafka in your terminal, type `exit`."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "kafka",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
